{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "68fiWA33TseY",
        "8I3KW7uBeoep",
        "dZCKzrnHevIJ",
        "6i8u7OTPfGyr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregate csv files from multiple experiments"
      ],
      "metadata": {
        "id": "4AcaJCLOT3H4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get report folders from Github\n",
        "Upload report files, get data from github repository, just specify experiment number to get files from, if you want to upload files manually skip these two cells or set the filed to 0"
      ],
      "metadata": {
        "id": "68fiWA33TseY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the experiment number to take files from\n",
        "# set 0 to skip and upload files manually\n",
        "experiment_number = 4"
      ],
      "metadata": {
        "id": "SzXVUvy4Rap2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "if experiment_number > 0:\n",
        "  # get the reports from GitHub\n",
        "  folder_path = f\"exp_{experiment_number}\"\n",
        "  repo_owner = \"asmeta\"\n",
        "  repo_name = \"asmeta\"\n",
        "  branch_name = \"isaac\"\n",
        "  folder_path = f\"code/experimental/asmeta.evotest/asmeta.evotest.experiments/data/exp_{experiment_number}\"\n",
        "\n",
        "  # GitHub URL API\n",
        "  api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}?ref={branch_name}\"\n",
        "\n",
        "  # Github API\n",
        "  response = requests.get(api_url)\n",
        "  subfolders = response.json()\n",
        "\n",
        "  # Loop through each subfolder\n",
        "  for subfolder in subfolders:\n",
        "      if subfolder[\"type\"] == \"dir\":\n",
        "          subfolder_url = subfolder[\"url\"]\n",
        "          response = requests.get(subfolder_url)\n",
        "          files = response.json()\n",
        "\n",
        "          # Download CSV files from the subfolder\n",
        "          for file in files:\n",
        "              if file[\"name\"].endswith(\".csv\"):\n",
        "                  file_url = file[\"download_url\"]\n",
        "\n",
        "                  # Create the directory if it doesn't exist\n",
        "                  file_path = os.path.join(\"/content/\", subfolder[\"name\"], file[\"name\"])  # Preserve subfolder structure\n",
        "                  os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "                  with open(file_path, \"wb\") as f:\n",
        "                      f.write(requests.get(file_url).content)"
      ],
      "metadata": {
        "id": "ga7FAbEgQHt5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the benchmark file"
      ],
      "metadata": {
        "id": "8I3KW7uBeoep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Compares benchmarks.csv files in subfolders and saves a consolidated file.\n",
        "\n",
        "first_benchmark = None  # To store the first benchmark file for comparison\n",
        "all_benchmarks_same = True  # Flag to track if all benchmarks are the same\n",
        "\n",
        "for subdir, dirs, files in os.walk(\"/content/\"):\n",
        "    # Skip subfolders that don't match the numeric pattern\n",
        "    if not re.fullmatch(r'\\d+', os.path.basename(subdir)):\n",
        "        continue\n",
        "    benchmark_path = os.path.join(subdir, 'benchmark.csv')\n",
        "    if os.path.exists(benchmark_path):\n",
        "        # Read the benchmark file\n",
        "        current_benchmark = pd.read_csv(benchmark_path)\n",
        "\n",
        "        # Compare with the first benchmark file\n",
        "        if first_benchmark is None:\n",
        "            first_benchmark = current_benchmark\n",
        "        elif not current_benchmark.equals(first_benchmark):\n",
        "            all_benchmarks_same = False\n",
        "            print(f\"Warning: Benchmark file in '{subdir}' is different.\")\n",
        "\n",
        "# Save the benchmark file to the root directory if all are the same\n",
        "if all_benchmarks_same and first_benchmark is not None:\n",
        "    first_benchmark.to_csv(os.path.join(\"/content/\", 'benchmark.csv'), index=False)\n",
        "    print(\"Benchmarks are the same. Saved to '/content/benchmark.csv'.\")\n",
        "else:\n",
        "    print(\"Benchmarks are different. No consolidated file saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MlIPdwDWWD3",
        "outputId": "3742a006-ad08-4a95-ae47-6dd500360fcd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarks are the same. Saved to '/content/benchmark.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregates execution time files"
      ],
      "metadata": {
        "id": "dZCKzrnHevIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Aggregates execution_time.csv files from subfolders and saves a consolidated file.\n",
        "\n",
        "all_data = []  # To store data from all execution_time.csv files\n",
        "\n",
        "for subdir, dirs, files in os.walk(\"/content/\"):\n",
        "    # Skip subfolders that don't match the numeric pattern\n",
        "    if not re.fullmatch(r'\\d+', os.path.basename(subdir)):\n",
        "        continue\n",
        "\n",
        "    execution_time_path = os.path.join(subdir, 'execution_time.csv')\n",
        "    if os.path.exists(execution_time_path):\n",
        "        # Read the execution_time.csv file\n",
        "        df = pd.read_csv(execution_time_path)\n",
        "        all_data.append(df)\n",
        "\n",
        "if all_data:\n",
        "    # Concatenate all data into a single DataFrame\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    # Calculate the mean of columns (excluding 'AsmSpec')\n",
        "    aggregated_df = combined_df.groupby('AsmSpec').agg({\n",
        "        'EvoAvalla': 'mean',\n",
        "        'Random': 'mean',\n",
        "        'Atgt': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Save the aggregated data to a new CSV file\n",
        "    aggregated_df.to_csv(os.path.join(\"/content/\", 'execution_time.csv'), index=False)\n",
        "    print(\"Aggregated execution times saved to '/content/execution_time.csv'.\")\n",
        "else:\n",
        "    print(\"No execution_time.csv files found in subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjrgR5ePdBe3",
        "outputId": "689992df-4c21-4f4d-fcf3-9cc95c147136"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated execution times saved to '/content/execution_time.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregates report files"
      ],
      "metadata": {
        "id": "6i8u7OTPfGyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import glob\n",
        "\n",
        "def aggregate_specific_report(report_file_name, root_dir='/content/'):\n",
        "    \"\"\"\n",
        "    Aggregates a specific report CSV file from subfolders and saves a consolidated file.\n",
        "\n",
        "    Args:\n",
        "        report_file_name: The name of the report CSV file to process (e.g., 'report_evoavalla.csv').\n",
        "        root_dir: The root directory to search for subfolders. Defaults to '/content/'.\n",
        "    \"\"\"\n",
        "    aggregated_data = {}  # To store aggregated data\n",
        "    experiments = 0 # number of experiments\n",
        "\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        # Skip subfolders that don't match the numeric pattern\n",
        "        if not re.fullmatch(r'\\d+', os.path.basename(subdir)):\n",
        "            continue\n",
        "\n",
        "        # Find the specific report CSV file in the subfolder\n",
        "        report_file_path = os.path.join(subdir, report_file_name)\n",
        "        if os.path.exists(report_file_path):\n",
        "            df = pd.read_csv(report_file_path)\n",
        "            experiments += 1\n",
        "\n",
        "            for index, row in df.iterrows():\n",
        "                key = (row['asm_name'], row['rule_signature'])\n",
        "\n",
        "                if key in aggregated_data:\n",
        "                    existing_data = aggregated_data[key]\n",
        "                    # Check if tot_conditional_rules and tot_update_rules are the same\n",
        "                    if (existing_data['tot_conditional_rules'][0] == row['tot_conditional_rules'] and\n",
        "                            existing_data['tot_update_rules'][0] == row['tot_update_rules']):\n",
        "                        existing_data['covered_true_conditional_rules'].append(row['covered_true_conditional_rules'])\n",
        "                        existing_data['covered_false_conditional_rules'].append(row['covered_false_conditional_rules'])\n",
        "                        existing_data['covered_update_rules'].append(row['covered_update_rules'])\n",
        "                    else:\n",
        "                        print(f\"Warning: Inconsistent tot_conditional_rules or tot_update_rules for key: {key}\")\n",
        "                else:\n",
        "                    aggregated_data[key] = {\n",
        "                        'execution_id': [\"aggregate_values_\" + row['asm_name']],\n",
        "                        'tot_conditional_rules': [row['tot_conditional_rules']],\n",
        "                        'tot_update_rules': [row['tot_update_rules']],\n",
        "                        'covered_true_conditional_rules': [row['covered_true_conditional_rules']],\n",
        "                        'covered_false_conditional_rules': [row['covered_false_conditional_rules']],\n",
        "                        'covered_update_rules': [row['covered_update_rules']],\n",
        "                        'failing_scenarios': [\"none\"]\n",
        "                    }\n",
        "\n",
        "    # Create a new DataFrame from the aggregated data\n",
        "    aggregated_df = pd.DataFrame(columns=['execution_id', 'asm_name', 'rule_signature', 'tot_conditional_rules',\n",
        "                                         'covered_true_conditional_rules', 'covered_false_conditional_rules',\n",
        "                                         'tot_update_rules', 'covered_update_rules', 'failing_scenarios'])\n",
        "\n",
        "    for key, data in aggregated_data.items():\n",
        "        aggregated_df = pd.concat([aggregated_df, pd.DataFrame({\n",
        "            'execution_id': data['execution_id'][0],\n",
        "            'asm_name': key[0],\n",
        "            'rule_signature': key[1],\n",
        "            'tot_conditional_rules': data['tot_conditional_rules'][0],\n",
        "            'covered_true_conditional_rules': [pd.Series(data['covered_true_conditional_rules']).sum() / experiments],\n",
        "            'covered_false_conditional_rules': [pd.Series(data['covered_false_conditional_rules']).sum() / experiments],\n",
        "            'tot_update_rules': data['tot_update_rules'][0],\n",
        "            'covered_update_rules': [pd.Series(data['covered_update_rules']).sum() / experiments],\n",
        "            'failing_scenarios': data['failing_scenarios'][0]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "    aggregated_df.to_csv(os.path.join(root_dir, report_file_name), index=False)\n",
        "    print(f\"Aggregated {report_file_name} saved to '/content/{report_file_name}'.\")\n"
      ],
      "metadata": {
        "id": "QvrthVQeizKp"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate Random Report"
      ],
      "metadata": {
        "id": "5ZjOhNCj06sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to execute with the specific report file name\n",
        "aggregate_specific_report('report_random.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpzNyGKFjN3c",
        "outputId": "f4134001-0949-429c-d08b-dfc78147cba6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated report_random.csv saved to '/content/report_random.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-cc78f00b289d>:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  aggregated_df = pd.concat([aggregated_df, pd.DataFrame({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate EvoAvalla Report"
      ],
      "metadata": {
        "id": "1XGXIYZY1Azg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to execute with the specific report file name\n",
        "aggregate_specific_report('report_evoavalla.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIB5-u9ojOz4",
        "outputId": "fa8dbf87-ab7c-4ef4-fc6f-88d3c44472b6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated report_evoavalla.csv saved to '/content/report_evoavalla.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-cc78f00b289d>:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  aggregated_df = pd.concat([aggregated_df, pd.DataFrame({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate Atgt Report"
      ],
      "metadata": {
        "id": "EWZj-wA31C-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to execute with the specific report file name\n",
        "aggregate_specific_report('report_atgt.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNgS_YHxjP2-",
        "outputId": "d21eec88-51ba-4ead-88d2-99680aa28e2b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-cc78f00b289d>:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  aggregated_df = pd.concat([aggregated_df, pd.DataFrame({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated report_atgt.csv saved to '/content/report_atgt.csv'.\n"
          ]
        }
      ]
    }
  ]
}